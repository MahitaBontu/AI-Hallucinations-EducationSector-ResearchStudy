# Synthetic AI Hallucinations as Adversarial Probes

This project analyzes how hallucinated inputs affect the behavior and plasticity of large language models (LLMs) in educational contexts.

## Models Used:
- GPT-3.5 (OpenAI)
- BERT (HuggingFace)
- DistilBERT (HuggingFace)
- RoBERTa (HuggingFace)

## Key Metrics:
- Baseline Accuracy
- Hallucination Agreement Rate
- Post Exposure Accuracy
- Feedback Loop Adaptation (FLA) Score

## How to Run:

### 1. Install Dependencies


### 2. Add OpenAI API Key
Update this line in the script:
```python
openai.api_key = "your-api-key"
python backend_llm_study.py
matplotlib for graphics

